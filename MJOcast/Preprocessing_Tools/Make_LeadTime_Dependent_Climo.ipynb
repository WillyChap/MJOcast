{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3665f276-5817-451d-b17d-74ba20f7ec5a",
   "metadata": {},
   "source": [
    "## What is this notebook?\n",
    "\n",
    "This book makes a lead time dependent climatology to use for any forecast system. \n",
    "\n",
    "Why Provide a Lead-Time Dependent Model Climatology in S2S Forecasting?\n",
    "\n",
    "In Subseasonal-to-Seasonal (S2S) forecasting, incorporating a lead-time dependent model climatology offers several advantages that contribute to improved forecast accuracy and reliability. Here’s why it’s considered a beneficial practice:\n",
    "\n",
    "- Reduced Biases: A standard climatology applied uniformly across all lead times might not account for the dynamic nature of atmospheric conditions, especially as a model slips into its own biased climatology, which is distinct from the real-world climatology. A lead-time dependent model climatology adapts to the evolving climate, mitigating biases that arise from using fixed observed climatological values.\n",
    "- Capturing Evolution: The climate system’s behavior changes as forecasts extend further into the future. A lead-time dependent climatology better captures this evolving nature, ensuring that the forecast is aligned with the expected conditions at various lead times.\n",
    "- Improved Skill: Incorporating a climatology that considers the lead time enhances the forecast’s skill by accounting for the removal of the biased evolution of the model’s climate. This leads to better capturing the evolving patterns, resulting in more accurate predictions.\n",
    "- Contextual Insights: A lead-time dependent climatology provides contextual insights into how the model’s climate system evolves over time. This understanding enhances the interpretation of forecast anomalies and aids in identifying significant departures from the norm.\n",
    "\n",
    "\n",
    "In summary, integrating a lead-time dependent model climatology acknowledges the changing nature of the model’s climate system and how it transitions into its own attractor space. By harnessing its variability, this approach enhances the accuracy and reliability of S2S forecasts. By adapting to evolving atmospheric conditions, this approach helps create more skillful and informative predictions.\n",
    "\n",
    "This preproccessing notebook allows a user to take their forecast and make a lead time dependent climatology in order to improve the skill of their forecasts. I strongly recommend the use of a dask cluster... otherwise this will take some time! \n",
    "\n",
    "Please adjust the \"settings\" markdown box below in order to use this notebook. \n",
    "\n",
    "The resulting NC file will be a lead time dependent climatology at every given day\n",
    "\n",
    "# What you will need to run this notebook:\n",
    "\n",
    "- 1) a directory with all of your forecast lead files in it... [/glade/scratch/wchapman/MJO_S2S_CESM2/p1/]\n",
    "- - Each File must be named by forecast lead time (i.e., S2Shindcast_cesm2cam6vs_MJOvars_10sep2018.nc)\n",
    "- 2) a certain amount of \"coverage\" of the files (i.e., you know how many files you need to make a CLIMO) it can't only be 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f5b7d-f2e4-4d5b-9f4d-38b9df650ee8",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16788f23-1c01-4ee2-bb23-ac83507a71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#import collections\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# import netCDF4\n",
    "# from netCDF4 import *\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy as cart\n",
    "import cartopy.mpl.ticker as cticker\n",
    "import cartopy.feature as cfeature\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "import glob\n",
    "import dask\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "from scipy.fftpack import ifft\n",
    "import copy\n",
    "import eofs.standard as Eof_st\n",
    "from eofs.multivariate.standard import MultivariateEof\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "import cmocean\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from pandas import Series\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a60739-6876-414d-b3b4-45e6d96c6b64",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d4d5c0c-47c9-43f0-b3c7-50a5d2385f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output directory\n",
    "outdir = '/glade/scratch/wchapman/MJO_S2S_CESM2/climo/'\n",
    "#output file pattern:\n",
    "Output_Fstring = 'MJOvars_climo_*.nc' #make sure you include a wild card (i.e. 'MJOvars_climo_*.nc')\n",
    "\n",
    "#directory where your forecast files are stored: \n",
    "dir_s2sfiles = '/glade/scratch/wchapman/MJO_S2S_CESM2/p1/'\n",
    "Fstring_s2sfiles = 'S2Shindcast_cesm2cam6vs_MJOvars_*.nc'\n",
    "\n",
    "#number of lead time days:\n",
    "num_leadays = 46\n",
    "\n",
    "#DAYS OF YEAR FOR THE CLIMO this is the starting day np.arange(1,367) does every day. ... \n",
    "# this can be a np.array with just the desired start date of year np.array([45,66,76])\n",
    "days_of_Y = np.arange(1, 367)\n",
    "\n",
    "string_format_forecast_File=\"%d%b\" \n",
    "# look up the strftime formats here: https://strftime.org/\n",
    "## if your files are of the form XXX_10sep2018.nc you will want %d%b to define the date as 10sep\n",
    "### the idea is to define this variable so that it recovers the day and month! \n",
    "####according to how you formated the ouput files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59804ed9-c695-4784-8371-df2dbd3cbb57",
   "metadata": {},
   "source": [
    "## Shut Down DASK Client If Necessarry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c6e981c-2557-46cb-bdc1-907bd3b00f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...shutdown client...\n"
     ]
    }
   ],
   "source": [
    "if 'client' in locals():\n",
    "    client.shutdown()\n",
    "    print('...shutdown client...')\n",
    "else:\n",
    "    print('client does not exist yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5a3c1-33c6-4932-868e-8dd9b67f7ad5",
   "metadata": {},
   "source": [
    "## Import Client for Dask Distributed Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63de7073-a47e-49c7-921c-bad56944da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(project='NAML0001',walltime='12:00:00')\n",
    "cluster.scale(40)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbabcfc1-274d-4f33-a057-65c86238ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900-01-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def get_dates_15_days_around(day_of_year, dminus, dplus):\n",
    "    # Get the current year\n",
    "    current_year = datetime.now().year\n",
    "    \n",
    "    # Create a date object for the given day of the year\n",
    "    date_obj = datetime.strptime(f\"{day_of_year:03d}\", \"%j\")\n",
    "    \n",
    "    # Generate the 15 days earlier and 15 days later dates\n",
    "    dates_list = []\n",
    "    print(date_obj)\n",
    "    for delta in range(dminus, dplus):\n",
    "        date = date_obj + timedelta(days=delta)\n",
    "        date_str = date.strftime(string_format_forecast_File).lower()\n",
    "        dates_list.append(date_str)\n",
    "\n",
    "    return dates_list,date_obj\n",
    "\n",
    "dates_list,date_obj = get_dates_15_days_around(10,-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448ad1f-07ea-4a9f-9a82-7a22c59ba4a1",
   "metadata": {},
   "source": [
    "## These are Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56f49a4-b20a-4420-a270-b4def84ad7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get dates 15 days before and after a given day of the year\n",
    "def get_dates_15_days_around(day_of_year, dminus, dplus):\n",
    "    # Get the current year\n",
    "    current_year = datetime.now().year\n",
    "    \n",
    "    # Create a date object for the given day of the year\n",
    "    date_obj = datetime.strptime(f\"{day_of_year:03d}\", \"%j\")\n",
    "    \n",
    "    # Generate the 15 days earlier and 15 days later dates\n",
    "    dates_list = []\n",
    "    for delta in range(dminus, dplus):\n",
    "        date = date_obj + timedelta(days=delta)\n",
    "        date_str = date.strftime(string_format_forecast_File).lower()\n",
    "        dates_list.append(date_str)\n",
    "\n",
    "    return dates_list\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def preprocess(ds):\n",
    "    # Set the 'time' coordinate to a new range from 0 to 45\n",
    "    ds['time'] = np.arange(0, 46)\n",
    "    return ds\n",
    "\n",
    "# Function to flatten a list of lists\n",
    "def flatten_list(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# Function to convert day of the year and year to a date string\n",
    "def dayofyear_to_date_string(year, dayofyear):\n",
    "    date = pd.to_datetime(f\"{year}-{dayofyear}\", format=\"%Y-%j\")\n",
    "    date_string = date.strftime('%Y-%m-%d')\n",
    "    return date_string\n",
    "\n",
    "def check_file_exists(filename):\n",
    "    return os.path.exists(filename)\n",
    "\n",
    "\n",
    "def adjust_number_forward(number):\n",
    "    if number > 366:\n",
    "        difference = number - 366\n",
    "        return difference\n",
    "    else:\n",
    "        return number\n",
    "    \n",
    "    \n",
    "def convert_dates_to_string(input_string):\n",
    "    # Define the regex pattern to match various date formats\n",
    "    date_pattern = r'(\\d{1,2}[A-Za-z]{3}\\d{4}|\\d{1,2}-[A-Za-z]{3}-\\d{4}|[A-Za-z]{3}[-_\\s]\\d{1,2}[-_\\s]\\d{4}|[0-3]?\\d[0-1]?\\d\\d{2}|[0-1]?\\d[0-3]?\\d\\d{2})'\n",
    "    \n",
    "    # Find all matches of the date pattern in the input string\n",
    "    try:\n",
    "        matches = re.findall(date_pattern, input_string)\n",
    "    except:\n",
    "        raise RuntimeError(\"the forecast files do not have a proper datestring as defined in the settings.yaml please change their names\")\n",
    "    \n",
    "    \n",
    "    # Loop through each matched date and convert it to the desired format\n",
    "    for match in matches:\n",
    "        # Parse the date string to a datetime object\n",
    "        try:\n",
    "            date_obj = datetime.strptime(match, '%b-%d-%Y')\n",
    "        except ValueError:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(match, '%d-%b-%Y')\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match, '%b_%d_%Y')\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        date_obj = datetime.strptime(match, '%b-%d-%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            date_obj = datetime.strptime(match, '%m%d%y')\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                date_obj = datetime.strptime(match, '%d%b%Y')\n",
    "                            except ValueError:\n",
    "                                continue  # Skip if the date format doesn't match any of the known formats\n",
    "        # Convert the datetime object to the desired format\n",
    "        formatted_date = date_obj.strftime('%d%b%Y')\n",
    "        \n",
    "        # Replace the matched date in the input string with the formatted date\n",
    "        input_string = input_string.replace(match, formatted_date)\n",
    "    \n",
    "    return input_string,matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99cfb5c-7147-4d52-a0cf-b2ad6f6fdbeb",
   "metadata": {},
   "source": [
    "## Run Part 1 \n",
    "\n",
    "This creates the lead time dependent files at each lead for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6b7e55-0296-4d09-94ff-25f20d54f891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day of year 1 of 366\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "#assertion: \n",
    "assert '*' in Output_Fstring, 'Output_Fstring must contain a wildcard i.e.: MJOvars_climo_*.nc'\n",
    "assert '*' in Fstring_s2sfiles, 'Fstring_s2sfiles must contain a wildcard i.e.: S2Shindcast_cesm2cam6vs_MJOvars_*.nc'\n",
    "\n",
    "print('the files will be placed here: ', dirout+Output_Fstring.split('*')[0]+'*'+Output_Fstring.split('*')[1])\n",
    "\n",
    "# Loop through days of the year (1 to 365) to process the data\n",
    "for ii in range(1, 367):\n",
    "    print(f\"day of year {ii} of 366\")\n",
    "    #### outfile \n",
    "    year_start = 1979\n",
    "    dayofyear_start = ii\n",
    "    date_string = dayofyear_to_date_string(year_start, dayofyear_start)\n",
    "    dirout = outdir\n",
    "    outfile = dirout+Output_Fstring.split('*')[0]+date_string+Output_Fstring.split('*')[1]\n",
    "    ####\n",
    "    \n",
    "    if check_file_exists(outfile):\n",
    "        print('file exists already')\n",
    "        continue \n",
    "    \n",
    "    # Record the start time for elapsed time calculation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get the 15 days before and after the current day of the year\n",
    "    result_dates = get_dates_15_days_around(ii, -15, 16)\n",
    "    \n",
    "    # Find files for the specified dates and concatenate them\n",
    "    FNs_dats = []\n",
    "    for rsrs in result_dates:\n",
    "        FNs_dats.append(sorted(glob.glob(dir_s2sfiles+Fstring_s2sfiles.split('*')[0]+rsrs+'*'+Fstring_s2sfiles.split('*')[1])))\n",
    "    FNs_dats = flatten_list(FNs_dats)\n",
    "    print(len(FNs_dats))\n",
    "    \n",
    "    # Open and preprocess the dataset using xarray\n",
    "    try:\n",
    "        DSmerge = xr.open_mfdataset(FNs_dats, parallel=True, combine='nested', concat_dim='bingo', preprocess=preprocess)\n",
    "    except:    \n",
    "        DSmerge = xr.open_mfdataset(FNs_dats, parallel=True, combine='nested', concat_dim='bingo', preprocess=preprocess)\n",
    "    \n",
    "    # Calculate the ensemble and temporal mean\n",
    "    DSmerge = DSmerge.mean(['bingo', 'ensemble']).squeeze()\n",
    "    \n",
    "    # Create a new coordinate 'dayofyear' and expand the dataset with it\n",
    "    data_newcoord = DSmerge.assign_coords(dayofyear='coord_value')\n",
    "    data_expanded = data_newcoord.expand_dims('dayofyear')\n",
    "    data_expanded['dayofyear'] = np.atleast_1d(ii)\n",
    "    \n",
    "    # Load the dataset into memory\n",
    "    data_expanded.load()\n",
    "    \n",
    "    # Update the 'time' coordinate with the correct dates\n",
    "    year_start = 1979\n",
    "    dayofyear_start = ii\n",
    "    date_string = dayofyear_to_date_string(year_start, dayofyear_start)\n",
    "    data_expanded['time'] = pd.date_range(start=date_string, periods=46, freq=\"D\")\n",
    "    data_expanded2 = copy.deepcopy(data_expanded)\n",
    "    # Record the end time and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Elapsed time: {elapsed_time:.6f} seconds after the load\")\n",
    "    \n",
    "    # Save or perform further processing\n",
    "    FNs_dats\n",
    "    data_expanded.attrs[\"forecasts_averaged_over\"] = FNs_dats\n",
    "    data_expanded.attrs[\"title\"] = \"MJO-related Climate Variables Climatology\"\n",
    "    data_expanded.attrs[\"source\"] = \"CESM2/CAM6 Model\"\n",
    "    data_expanded.attrs[\"institution\"] = \"National Center for Atmospheric Research (NCAR)\"\n",
    "    data_expanded.attrs[\"references\"] = \"Will Chapman. (2023), Journal of Climate, Vol. XX, pp. XX-XX.\"\n",
    "    data_expanded.attrs[\"description\"] = \"This dataset contains MJO-related climate variables.\"\n",
    "    data_expanded.attrs[\"author\"] = \"Will Chapman\"\n",
    "    data_expanded.attrs[\"creation_date\"] = \"2023-07-28\"\n",
    "    data_expanded.attrs[\"contact\"] = \"Will Chapman (wchapman@ucar.edu)\"\n",
    "    data_expanded.to_netcdf(outfile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050c4b7-e9df-44fd-af6c-302100879c0e",
   "metadata": {},
   "source": [
    "## Compile based on lead time:\n",
    "\n",
    "This block compiles all of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91251d24-e85d-4e58-b4f0-1c433d014801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fnfn = sorted(glob.glob(outdir + Output_Fstring ))\n",
    "DDS = xr.open_dataset(fnfn[10])\n",
    "outdir = outdir\n",
    "for ldld in np.arange(0,num_leadays):\n",
    "    outfile = outdir+'MJOvars_lead'+f\"{ldld:0{3}d}\"+'_climo.nc'\n",
    "    if check_file_exists(outfile):\n",
    "        print('file exists already')\n",
    "        continue \n",
    "        \n",
    "    print(ldld)\n",
    "    start_time = time.time()\n",
    "    for ee,FN in enumerate(fnfn):\n",
    "        DDS=xr.open_dataset(FN)\n",
    "        #open the files and adjust the dates forward in time:\n",
    "        if ee == 0:\n",
    "            DDSmerge = DDS.isel(time=ldld).drop('time')\n",
    "            DDSmerge['dayofyear']= np.atleast_1d(adjust_number_forward(DDS.isel(time=ldld)['dayofyear'].values[0]+ldld))\n",
    "        \n",
    "        else: \n",
    "            DDStemp = DDS.isel(time=ldld).drop('time')\n",
    "            DDStemp['dayofyear']=np.atleast_1d(adjust_number_forward(DDStemp['dayofyear'].values[0]+ldld))\n",
    "            DDSmerge = xr.concat([DDSmerge,DDStemp],dim='dayofyear') #we are hoping this concatenates and respects the dayofyear index\n",
    "    #sort by the day of year... \n",
    "    DDSmerge_sort = DDSmerge.sortby('dayofyear')   \n",
    "    #roll the result \n",
    "    DDSmerge_roll=DDSmerge_sort.rolling(dayofyear=20,center=True,min_periods=1).mean()\n",
    "    \n",
    "    DDSmerge_roll.to_netcdf(outdir+'MODELVARS_lead'+f\"{ldld:0{3}d}\"+'_climo.nc')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.6f} seconds after the load\")\n",
    "    print(outfile)\n",
    "FNS = sorted(glob.glob(outdir+'MODELVARS_lead'+'*_climo.nc'))\n",
    "DS_climo = xr.open_mfdataset(FNS,combine='nested',concat_dim='lead').assign_coords(coords={'lead':np.arange(0,46)})\n",
    "DS_climo.to_netcdf(outdir+'/MODEL_VARS_full_climo.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2023b",
   "language": "python",
   "name": "npl-2023b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
